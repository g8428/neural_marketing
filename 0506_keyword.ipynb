{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "06HDiFQ0cKkc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"0425_ENG_final_drama_reviews.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "X_PKit713vdq",
        "outputId": "e5c6d2c8-d7af-4584-de20-be7b7f9d0968"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                          18어게인\n",
              "1                  365:운명을거스르는1년\n",
              "2                       60일지정생존자\n",
              "3                           D.P.\n",
              "4     검색어를입력하세요WWW\n",
              "                 ...            \n",
              "87                  하늘에서내리는일억개의별\n",
              "88                          하이에나\n",
              "89                      한번다녀왔습니다\n",
              "90                          해피니스\n",
              "91                         호텔델루나\n",
              "Name: title, Length: 92, dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drama_titles = df.groupby('title',as_index=False).count().title\n",
        "drama_titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7BPRLR5l3tJ6"
      },
      "outputs": [],
      "source": [
        "reviews = df.reviews.to_list()\n",
        "sentences = \",\".join(reviews)\n",
        "reviews_by_drama = []\n",
        "for title in titles:\n",
        "    rev_by_drama = df[df.title == title].reviews\n",
        "    reviews_by_drama.append(','.join(rev_by_drama))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "182687"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAvj4QvycKkg"
      },
      "source": [
        "# 필요 함수들"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqAn9_AudeEC"
      },
      "outputs": [],
      "source": [
        "# pip install contextualized_topic_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Uzcta1LzcKki"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "# from contextualized_topic_models.models.ctm import CombinedTM\n",
        "# from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation, bert_embeddings_from_list\n",
        "from contextualized_topic_models.utils.preprocessing import WhiteSpacePreprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "# from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za3iZ_T5cKkj"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(('\\r\\n', '\\n\\r', 'ever', 'much', 'look', 'squid', 'show', 'thing', \"i've\", 'anything', 'something', \"show's\",\n",
        "                   'www', 'soompi' 'com', 'instagram', 'youtube', 'https', 'mydramalist', 'twitter', 'episode', 'comment', 'scene',\n",
        "                   'version', \"he's\", 'gonna', 'series', 'watch', 'everything', 'something', \"can't\", 'list', 'dramas', 'drama',\n",
        "                   'wait', 'preview', 'someone', 'everyone', 'dont', 'think', 'season', 'anyone', 'something', 'anything', 'nothing', 'world',\n",
        "                   'status', 'week', 'name', 'cause', 'time', 'en', 'org', 'wikipedia', 'wiki', 'pbs', 'twimg', 'year', 'point', 'please', 'today',\n",
        "                   'haha', 'case', 'guess', 'reason', 'person', 'moment', 'sense', 'kinda', 'part', 'movie', 'school', 'start', 'work', 'lead', 'kind',\n",
        "                   'rate', 'rating', 'rate', 'men', 'example', 'idea', 'half', 'review', 'genre', 'side', \"that's\", \"they're\", 'till', 'tell', 'phone',\n",
        "                   'section', 'number', 'company', 'line', \"there's\", 'male', 'team', 'rating', 'baby', 'course', 'care', 'cute', 'question', 'help', 'group',\n",
        "                   'hand', 'spoiler', 'hate', 'need', 'mess', 'change', 'drop', 'date', 'netflix', 'yeah', 'daon', 'park', 'thank', 'lmao', 'damn', \"i'll\",\n",
        "                   'kang', 'shinwoo', 'taekyung', 'mean', 'woman', 'hope', 'read', 'fact', 'opinion', 'stuff', 'feel', 'kdrama', 'talk', 'song', 'hype',\n",
        "                   'title', 'type'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 사용 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FFx6w8wQcKkj"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(pos_tag):\n",
        "    if pos_tag.startswith('V'):\n",
        "        return 'v'\n",
        "    elif pos_tag.startswith('N'):\n",
        "        return 'n'\n",
        "    elif pos_tag.startswith('J'):\n",
        "        return 'a'\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return 'r'\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CustomTokenizer class setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q6PzBOOfcKkk"
      },
      "outputs": [],
      "source": [
        "class CustomTokenizer:\n",
        "    def __init__(self, tagger):\n",
        "        self.tagger = tagger\n",
        "    def __call__(self, a):\n",
        "        # a = ' '.join(a)\n",
        "        word_tokens = self.tagger(a)\n",
        "        \n",
        "        words = []\n",
        "        for i in word_tokens:\n",
        "            text = re.sub('[^a-zA-Z0-9\\']','',i).strip()\n",
        "            text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`…》]','', text)\n",
        "            if(text != ''):\n",
        "                words.append(text)\n",
        "        \n",
        "        tag_words = nltk.pos_tag(words)\n",
        "        pos_words = [word for word in tag_words if word[1][0] in {'N'}] #'V','N','J','R'\n",
        "\n",
        "        temp_list = []\n",
        "        for token, pos_tag in pos_words:\n",
        "            tag = get_wordnet_pos(pos_tag)\n",
        "            if tag != None:\n",
        "                temp_list.append((token, get_wordnet_pos(pos_tag)))\n",
        "        lemma = WordNetLemmatizer()\n",
        "        token_final = [lemma.lemmatize(token, pos=tag) for token, tag in temp_list]\n",
        "        long_words = [i for i in token_final if len(i) > 2]\n",
        "        results = [w for w in long_words if w not in stop_words]\n",
        "        \n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUITwIWzcKkl"
      },
      "source": [
        "# 작업"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUf7tKGGcKkl",
        "outputId": "7916a4e6-62cf-4545-8f50-57a2d8fa8732"
      },
      "outputs": [],
      "source": [
        "def tfidf_vectorizing(reviews):\n",
        "    ngram_range = (1,2)\n",
        "\n",
        "    custom_tokenizer = CustomTokenizer(text_to_word_sequence)\n",
        "\n",
        "    tfidf = TfidfVectorizer(tokenizer=custom_tokenizer,ngram_range = ngram_range,\\\n",
        "        stop_words = stop_words, max_df=10, min_df=2 ,max_features=5000).fit(reviews)\n",
        "    candidates = tfidf.get_feature_names()\n",
        "    \n",
        "    return candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcJtDtpicKkm"
      },
      "source": [
        "# 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AGKQ1FmHcKkm"
      },
      "outputs": [],
      "source": [
        "def embedding(sentences,candidates):\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "    model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "    doc_embedding = model.encode([sentences])\n",
        "    candidate_embeddings = model.encode(candidates)\n",
        "    # drama_embedding = model.encode(reviews_by_drama)\n",
        "\n",
        "    return doc_embedding, candidate_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(92, 768)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drama_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsW6mgMi4N71"
      },
      "source": [
        "# 코사인 유사도 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wIJ9WZpmlqlu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "top_n = 500\n",
        "diversity = 0.6\n",
        "\n",
        "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
        "    word_doc_distances = cosine_similarity(candidate_embeddings,doc_embedding)\n",
        "    word_distances = cosine_similarity(candidate_embeddings) \n",
        "\n",
        "    keywords_idx = [np.argmax(word_doc_distances)]\n",
        "    candidates_idx = [i for i in range(len(candidates)) if i != keywords_idx[0]]\n",
        "    \n",
        "    for _ in range(top_n-1):\n",
        "        try:\n",
        "        # 후보 키워드들의 문서유사도 값\n",
        "            candidate_similarities = word_doc_distances[candidates_idx, :]\n",
        "            # 후보 키워드와 가장 유사한 키워드\n",
        "            target_similarities = np.max(word_distances[candidates_idx][:,keywords_idx], axis=1) \n",
        "\n",
        "            mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1,1)\n",
        "            mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "            keywords_idx.append(mmr_idx)\n",
        "            candidates_idx.remove(mmr_idx)\n",
        "        except:\n",
        "            break\n",
        "\n",
        "    # 본 단어 임베딩벡터에서 추출키워드에 해당하는 벡터만\n",
        "    keywords_vector = candidate_embeddings[keywords_idx][:] \n",
        "    keywords = [words[idx] for idx in keywords_idx]\n",
        "    \n",
        "    return keywords, keywords_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18어게인\n",
            "['character heartwarming', 'relationship flaw', 'grandma killer', 'chemistry butterfly', 'eatmelonn', 'rain way', 'playbook hospital', 'pain guilt', 'people battle', 'crime love'] \n",
            "\n",
            "365:운명을거스르는1년\n",
            "['charm actor', 'mom tae', 'disease zombie', 'fencing match', 'dramaaddictscorner com', 'sad bittersweet', 'story heartwarming', \"jin's life\", 'girlfriend love', 'cop killer'] \n",
            "\n",
            "60일지정생존자\n",
            "['boring people', 'hyun dream', 'chemistry butterfly', 'brother murder', 'happiness zombie', \"drama's success\", 'romance conflict', 'game commentary', 'anxiety attack', 'kdramas ending'] \n",
            "\n",
            "D.P.\n",
            "['pain actor', 'girl chemistry', 'happiness zombie', 'brother hyun', 'story heartwarming', 'cinematography transition', 'dramaaddictscorner com', 'killer mom', 'chicken restaurant', 'misunderstanding couple'] \n",
            "\n",
            "검색어를입력하세요WWW\n",
            "['vibe couple', 'korea zombie', 'plot inconsistency', 'girl tae', 'chicken restaurant', 'dramaaddictscorner com', 'job dream', 'taste preference', 'actor soo', 'chemistry butterfly'] \n",
            "\n",
            "무브투헤븐:나는유품정리사입니다\n",
            "['vibe trailer', 'wife problem', 'chemistry butterfly', 'writer jung', 'cannibalism', \"headhunter's son\", 'others mistake', 'trauma lot', 'love prosecutor', 'cast doctor'] \n",
            "\n",
            "보좌관:세상을움직이는사람들\n",
            "['murder corruption', 'han jbr', 'struggle romance', 'character heartwarming', 'nietzsche', 'game hellbound', 'politics family', 'doctor actor', 'ignorant', 'taxi rainbow'] \n",
            "\n",
            "갯마을차차차\n",
            "['charm actor', 'death girl', 'tea plot', 'anxiety fear', 'boring people', 'break romance', 'game commentary', 'brain cancer', 'lds lds', 'binge lot'] \n",
            "\n",
            "괴물\n",
            "['boring people', 'reboot', 'dramaaddictscorner com', 'happiness zombie', 'family dinner', 'romance pacing', 'murder sister', 'lingering feeling', 'game commentary', 'bit overdone'] \n",
            "\n",
            "구경이\n",
            "['character insecurity', 'tae butterfly', 'happiness zombie', 'game commentary', 'hospital mother', 'law thriller', 'bit overdone', 'stabbing', 'taste preference', 'cry acting'] \n",
            "\n",
            "그녀의사생활\n",
            "['dream mother', 'actor pace', 'korea zombie', 'character insecurity', 'tae butterfly', 'game commentary', 'story heartwarming', 'playbook hospital', 'mind theory', 'rain way'] \n",
            "\n",
            "그림자미녀\n",
            "['character insecurity', 'taylor swift', 'happiness zombie', 'parent chicken', 'business day', 'witch hunt', 'epilogue end', 'doctor surgery', 'story heartwarming', 'sniper'] \n",
            "\n",
            "기름진멜로\n",
            "['woo crush', 'dishwasher', 'mastermind game', 'borderland hunger', 'chemistry butterfly', 'policeman brother', 'cinematography wise', 'dramaaddictscorner com', 'parent chicken', 'taylor swift'] \n",
            "\n",
            "김비서가왜그럴까\n",
            "['story disappoint', 'zombie life', 'binge eps', 'parent chicken', 'game commentary', 'witch hunt', 'plan day', 'mind mom', 'pacing acting', 'psychopath doctor'] \n",
            "\n",
            "나빌레라\n",
            "['story disappoint', 'goryeo', 'chemistry butterfly', \"dan's mother\", 'happiness zombie', 'game commentary', 'girl murder', 'dramaaddictscorner com', 'family dinner', 'job dream'] \n",
            "\n",
            "나의아저씨\n",
            "['love heartbreak', 'viewer acting', 'goblin descendant', 'boring people', 'chemistry butterfly', 'healing journey', 'raincoat killer', 'mom flashback', 'student friend', 'god game'] \n",
            "\n",
            "낭만닥터김사부\n",
            "['hospital romance', \"i'm manhwa\", 'plot inconsistency', 'rewatch story', 'pacing acting', 'mother killer', 'family dinner', 'others fan', 'zombie joseon', 'directing production'] \n",
            "\n",
            "내뒤에테리우스\n",
            "['happiness zombie', \"dan's mother\", 'flower comic', 'crime lover', 'random people', 'game commentary', 'family dinner', 'cast concept', 'love kyung', 'apartment happiness'] \n",
            "\n",
            "녹두꽃\n",
            "['story disappoint', 'brother hyun', 'disease zombie', 'fan others', 'pacing viewer', 'perfection actor', 'cop game', 'chemistry god', 'dramaaddictscorner com', 'wife ceo'] \n",
            "\n",
            "단하나의사랑\n",
            "['soo actress', 'borderland hunger', 'mastermind game', 'heart comedy', 'parent chicken', 'boring people', 'kid crime', 'cry acting', 'zombie joseon', 'grandma killer'] \n",
            "\n",
            "대박부동산\n",
            "['boring people', 'reboot', 'killer hospital', 'eat dog', 'fencing match', 'mastermind game', 'baekdo end', 'heart forever', \"mother's murder\", 'plot inconsistency'] \n",
            "\n",
            "도시남녀의사랑법\n",
            "['character insecurity', 'chicken restaurant', 'taylor swift', 'production love', 'rewatch story', 'korea zombie', 'cinderella story', 'chemistry butterfly', 'law thriller', 'psychology people'] \n",
            "\n",
            "동백꽃필무렵\n",
            "[\"drama's success\", 'chemistry butterfly', 'brother dream', 'korea zombie', 'wife problem', 'game commentary', 'finger killer', 'love closure', 'chicken restaurant', 'power hungry'] \n",
            "\n",
            "라이브\n",
            "['story coincidence', 'boring people', 'vibe kdramas', 'bit overdone', 'character directing', 'love prosecutor', 'family dinner', \"let's pray\", 'boy wolf', 'actress others'] \n",
            "\n",
            "라이프온마스\n",
            "['love script', 'zombie period', 'boring people', 'brother hyun', 'fencing match', 'psychopath doctor', 'thriller viewer', 'dramaaddictscorner com', 'heart ost', 'girl chemistry'] \n",
            "\n",
            "라켓소년단\n",
            "['charm plot', 'film parasite', \"han's mother\", 'blood sweat', 'action personality', 'character heartwarming', 'sport badminton', 'family dinner', 'killer mouse', 'jun surgery'] \n",
            "\n",
            "런온\n",
            "['lover chemistry', 'emotion sadness', 'manhwa adaptation', 'girl murder', 'game commentary', 'pace acting', 'plot inconsistency', 'family dinner', 'space story', 'college exam'] \n",
            "\n",
            "로맨스는별책부록\n",
            "['lover chemistry', 'girl cry', 'family acting', 'zombie apartment', 'dreamt', 'story heartwarming', 'law jargon', 'boring people', 'box kleenex', 'hospital friend'] \n",
            "\n",
            "로스쿨\n",
            "[\"drama's success\", 'wife problem', 'disease zombie', 'chemistry butterfly', 'brother dream', 'power hungry', 'kdramas info', 'rewatch story', 'law jargon', 'logic people'] \n",
            "\n",
            "마녀식당으로오세요\n",
            "['creepy smile', 'tae mom', 'chicken restaurant', \"let's pray\", 'struggle job', 'chemistry butterfly', 'life playlist', 'romance conflict', 'boy wolf', 'midnight runner'] \n",
            "\n",
            "마우스\n",
            "['character insecurity', 'chemistry butterfly', \"i'd plot\", 'disease zombie', 'dream mother', 'value cinematography', 'pacing acting', 'fan page', 'feeling husband', 'writer crew'] \n",
            "\n",
            "멜로가체질\n",
            "['charm actor', \"mother's murder\", 'borderland hunger', 'soo family', 'story misunderstanding', 'chemistry butterfly', 'boring people', 'playbook hospital', 'brother tear', 'life challenge'] \n",
            "\n",
            "모범택시\n",
            "['crime love', 'dishwasher', 'webtoon taemu', 'wife problem', 'experience acting', 'attack titan', 'taxi guy', 'flower comic', 'reincarnation story', 'burger'] \n",
            "\n",
            "미스터션샤인\n",
            "[\"drama's success\", 'family dinner', 'taylor swift', 'mindhunters', 'relationship flaw', 'baekdo end', 'anxiety fear', 'job dream', 'lds killer', 'sang train'] \n",
            "\n",
            "미스티\n",
            "['perfect actor', 'witch hunt', 'break lot', 'tae butterfly', 'zombie joseon', 'dramaaddictscorner com', 'girlfriend love', 'lot soundtrack', 'power hungry', 'mastermind game'] \n",
            "\n",
            "미치지않고서야\n",
            "['happiness zombie', 'therapy people', 'plot politics', 'dream mother', 'job fun', 'college exam', 'character insecurity', 'minute acting', 'chicken restaurant', 'bit overdone'] \n",
            "\n",
            "백일의낭군님\n",
            "['charm actor', 'mother news', 'com eatmelonn', 'pacing acting', 'murder vibe', 'student lot', 'emotion rollercoaster', 'story heartwarming', 'couple jae', 'happiness zombie'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# for idx, drama in enumerate(drama_embedding):\n",
        "#     keywords, keywords_vector = mmr(drama.reshape(1,-1) ,candidate_embeddings,candidates,top_n,diversity)\n",
        "#     print(titles[idx])\n",
        "#     print(keywords[:10],'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### keyword Dataframe 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zrZeLhwA0Kmx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "1it [00:28, 28.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18어게인 ['character heartwarming', 'relationship misunderstanding', 'prison playbook', 'story chemistry', 'eighteen tearjerker', 'pandemic', 'family sacrifice']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "2it [00:51, 25.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "365:운명을거스르는1년 ['officer cuteness', 'tae sister', \"foreshadowing pen's\", 'killing trophy', 'chemistry thriller', 'quackery plot', 'actor actress']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "3it [01:07, 21.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60일지정생존자 ['awkward romance', 'survivor day', 'hee actor', 'lazy', 'people politics', 'production cinematography', 'antagonist']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "4it [01:29, 21.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D.P. ['fun pain', 'actress jeon', 'chemistry actor', 'bullying soldier', 'midnight runner', 'army deserter', 'spotify com']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "5it [01:54, 22.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "검색어를입력하세요WWW ['romance irritating', 'sunbae life', 'jung actor', 'reservation honest', 'pianist girl', 'spotify com', 'story development']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "6it [02:18, 23.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "무브투헤븐:나는유품정리사입니다 [\"asperger's character\", 'parent love', 'cage fighting', 'memory creepy', 'lee jehoon', 'girl butterfly', 'development story']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "7it [02:31, 19.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "보좌관:세상을움직이는사람들 ['judgemental politician', 'romance story', 'corruption politics', 'kdramas', 'tae jun', 'coherent party', 'power struggle']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "8it [03:39, 35.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "갯마을차차차 ['love actor', 'death grandma', 'thought rice', 'shopaholic donator', 'hypocrisy', 'july', 'wedding dress']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "9it [04:36, 42.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "괴물 ['viewer suspense', \"sister's body\", 'soul mechanic', 'buzzkpop com', 'one fishing', 'supermarket owner', 'cop murder']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "10it [04:54, 34.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "구경이 ['disgust rationalisation', 'gay energy', 'mouse chase', 'actress character', 'fun crime', 'cup tea', 'student killer']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "11it [05:26, 33.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "그녀의사생활 ['love acting', 'birth nonsense', 'band night', 'police mother', 'gay baker', 'life romcom', 'remorse bullcrap']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "12it [05:42, 28.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "그림자미녀 ['character creepy', 'eps day', 'relationship mom', 'surgery', 'sleepless', 'screenwriter', 'online life']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "13it [06:01, 25.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "기름진멜로 ['woo romance', 'gangster cook', 'evil mother', 'writer incarnate', 'horse cancer', 'trust commenters', 'payback cutest']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "14it [06:36, 28.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "김비서가왜그럴까 ['suspense story', 'min chemistry', 'trailer judge', 'healer game', 'brother girl', 'pollen allergy', 'lack plot']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "15it [06:58, 26.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "나빌레라 ['happiness sadness', 'taemin', 'dancer lot', 'dad plot', 'manga butterfly', 'nature ups', 'experience grandma']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "16it [07:38, 30.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "나의아저씨 ['love betrayal', 'grocery', 'drinking dialogue', 'job sun', 'girl hardship', 'people wiretapping', 'value people']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "17it [07:56, 26.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "낭만닥터김사부 ['hospital rivalry', 'rerun', 'greedy', 'pacing lot', 'doldam crew', 'felt romance', 'mother']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "18it [08:14, 23.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "내뒤에테리우스 ['mind romance', 'sandwich', 'actor spy', 'family comedy', 'nobody director', 'tae agent', 'mix thriller']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "19it [08:27, 20.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "녹두꽃 ['story romance', 'brother hyun', 'rougthless politician', 'watching', 'teacher', 'thief', 'politician intensity']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "20it [08:53, 22.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단하나의사랑 ['weird angel', 'ballet bottom', 'character budget', \"kim dan's\", 'tragedy tragedy', 'overdone', 'aunt cousin']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "21it [09:15, 22.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "대박부동산 ['pain agony', 'actress jang', 'business ghost', 'chemistry cast', 'rest plot', 'yugami blood', 'dramacool']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "22it [10:07, 31.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "도시남녀의사랑법 ['lovestruck acting', 'girl month', 'relationship fear', 'domesticviolence ptsd', 'jaewon alcoholism', 'flashback beach', 'medicalnewstoday com']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "23it [10:33, 29.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "동백꽃필무렵 ['love acting', 'son dam', 'plot lol', 'boyfriend wife', 'mind mom', 'bloom blend', 'life lesson']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "24it [10:49, 25.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "라이브 ['character importance', 'chemistry wook', \"'guys' dream\", 'spotify com', 'pacing', 'mother alcoholic', 'soundtrack romance']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "25it [11:09, 23.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "라이프온마스 ['thriller suspense', 'brother min', 'victim manicure', 'hospital', 'love jung', 'prison playbook', 'dream hallucination']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "26it [11:31, 23.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "라켓소년단 ['badminton fun', 'scriptwriter', 'family dynamic', \"i'm rewatch\", 'share indonesia', 'student baseball', 'lover korean']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "27it [12:36, 35.83s/it]c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "런온 ['romance boring', 'development chemistry', 'filter people', 'reality film', 'prison playbook', 'pacing storytelling', 'politics family']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "28it [13:01, 32.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "로맨스는별책부록 ['romance heartwarming', 'shame dan', 'bookworm book', 'chemistry brother', 'break motherhood', 'eun character', 'villain refreshing']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "29it [13:30, 31.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "로스쿨 ['fun law', 'sister people', 'doctor exam', 'mastermind murder', 'chill goosebump', 'script storyline', \"drama's ost\"]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "30it [13:46, 26.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "마녀식당으로오세요 ['witch wish', 'abuse tension', 'hyun actor', 'food vibe', 'life lesson', 'chemistry character', 'plot gist']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "31it [14:59, 40.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "마우스 ['killer coincidence', 'surgery writer', 'girl family', 'lack empathy', 'prey vlogger', 'train thought', 'character chemistry']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "32it [15:17, 33.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "멜로가체질 [\"jung's brother\", 'idiocy heartfelt', 'character problem', 'girl friendship', 'drunk', 'comedy challenge', 'boyfriend restaurant']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "33it [15:51, 33.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모범택시 ['love taxi', 'heist', 'story vigilante', 'hacker girl', 'stunt interview', 'revenge romance', 'superhero']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "34it [16:32, 35.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "미스터션샤인 ['happiness tragedy', 'tae actress', 'butcher', 'goosebump', 'screenwriter couple', 'turbulent history', 'experience cinematography']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "35it [16:51, 30.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "미스티 ['cast superb', 'abortion', \"wook's father\", 'business law', 'misty evil', 'people dream', 'studio wife']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "36it [17:08, 26.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "미치지않고서야 ['favorite plot', 'negativity workforce', 'politics story', 'choi miss', 'applause job', 'insanity challenge', 'dinner']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "37it [17:32, 26.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "백일의낭군님 ['love acting', 'shopping king', 'king louie', 'day burn', \"seo's brother\", 'music rewatch', 'wonder emptiness']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "38it [17:49, 23.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "본대로말하라 ['wit villain', 'teen college', 'fight police', 'herring', 'manga murder', 'couple revelation', 'moonoverstar']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "39it [18:20, 25.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "부부의세계 ['cheat wife', 'audacity taeoh', 'train wreck', 'calm storm', 'prison playbook', 'dad lot', \"shy drama's\"]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "40it [18:40, 23.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "뷰티인사이드 ['power actress', 'dream priest', 'com dramafoxblog', 'plot twist', 'intense boring', 'love chemistry', 'rewatch']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "41it [19:41, 35.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "비밀의숲 ['stranger misogyny', 'television teamwork', 'repeat essay', 'perfect policewoman', \"simok yeojin's\", 'prison playbook', 'ordeal evidence']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "42it [20:55, 46.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사내맞선 ['cringe acting', 'ceo girl', 'restaurant kiss', 'grandfather blind', 'zombie crime', 'scissors game', 'breakup trope']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "43it [22:03, 53.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사랑의불시착 ['love cast', 'accident korea', 'wife antagonist', 'philosopher tomato', 'businessinsider tvns', 'yawn', 'life lesson']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "44it [22:18, 41.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사의찬미 ['tragedy cheating', 'bgm album', 'typewriter', 'lack chemistry', 'rewatch', 'script love', 'pressure society']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "45it [23:43, 54.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사이코지만괜찮아 ['character heartache', 'kim moon', 'love chemistry', 'prison playbook', \"ji's acting\", 'photo shoot', 'butterfly killer']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "46it [24:07, 45.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "서른아홉 ['friend actress', 'execution lot', \"trailer netflix's\", 'chemistry son', 'heartache', 'jeon haircut', 'thirty subtitleshttps']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "47it [24:27, 38.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "서른이지만열일곱입니다 ['character charm', 'cry laugh', 'development story', 'bus accident', 'chemistry music', 'cinderella knight', 'hae sun']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "48it [25:44, 49.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "세빛남고학생회 ['shyness insecurity', 'interview endgame', 'taedaon taeshinwoo', 'chemistry butterfly', 'feeling home', 'fond soundtrack', 'destiny']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "49it [26:02, 40.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "소년심판 ['acting lot', 'end rape', 'lesson backstories', 'kim hyesoo', 'ruling juvenile', 'binge day', 'fan law']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "50it [26:26, 35.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "손theguest ['supernatural', 'character development', 'bos fight', 'actor lot', 'yang finale', 'chemistry love', 'filming fear']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "51it [26:40, 29.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "술꾼도시여자들 ['girl friendship', 'heist', 'lot drinking', 'kdramas', 'comedy amount', 'slice life', 'sickness health']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "52it [28:09, 46.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "스물다섯스물하나 ['relationship cringe', 'fullhouse manhwa', \"heedo's daughter\", 'endgame writer', 'color theory', 'evolution character', 'problem goblin']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "53it [28:33, 40.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "스카이캐슬 ['character manipulative', 'nature parent', 'sky university', 'money flower', 'homeroom', 'badass bunny', 'romance romance']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "54it [28:51, 33.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "스토브리그 ['baseball problem', 'passion leadership', 'prison playbook', 'imdb', 'script draggy', 'cinematography shot', 'league viewer']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "55it [29:56, 42.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "슬기로운의사생활 ['love writer', 'marathoner', 'scare ikjun', 'spam bot', 'kingdom lockdown', 'dinner mother', 'people plot']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "56it [30:12, 34.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "시를잊은그대에게 ['lot comedy', 'nurse trainee', 'heart sore', 'woo alligator', 'income dream', 'kim dae', 'relationship romance']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "57it [30:53, 36.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "시맨틱에러 ['cuteness overload', \"director's cut\", 'jaeyoung restaurant', 'manhwa novel', 'chemistry butterfly', 'fight minute', 'stalking']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "58it [31:14, 31.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "아는와이프 ['heart cuteness', 'husband divorce', 'management fault', 'actor korea', \"'destiny'\", 'lot cast', 'fraud money']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "59it [32:21, 42.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "악의꽃 ['suspense thrill', \"'mother' flower\", 'lawless lawyer', 'hospital accident', 'trailer mind', 'chemistry actor', 'cake acting']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "60it [32:41, 35.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "악의마음을읽는자들 ['korean mindhunter', 'day crime', 'lullaby', 'handsome actor', 'nonfiction', 'others mind', 'procedure profiling']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "61it [32:59, 30.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "안녕나야 ['fun character', 'truck doom', 'wedding day', \"drama's\", 'actor chemistry', 'development lukewarm', 'story romance']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "62it [34:05, 41.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "어느날우리집현관으로멸망이들어왔다 ['disappoint story', 'actress chemistry', 'plot goblin', 'soul actor', 'family watching', 'angel death', 'wrenching romance']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "63it [34:59, 44.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "어쩌다발견한하루 ['romance acting', 'mind day', 'webtoon bittersweet', 'others manhwa', 'brother fight', 'stage lot', 'twist dan']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "64it [35:31, 41.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "오월의청춘 ['love actor', 'tae girl', 'hymn death', 'storytelling', 'selfish', 'student law', 'month romance']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "65it [36:47, 51.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "오징어게임 ['character favorite', 'runner hunger', 'capitalism illusion', 'game anxiety', 'film parasite', 'twist finale', 'senseless violence']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "66it [37:58, 57.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "옷소매붉은끝동 ['politics romance', \"i'm rewatch\", 'sad actor', 'palace ritual', 'story joseon', 'power revenge', 'machine translation']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "67it [38:13, 44.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "왓쳐 ['suspense plot', 'kdramas', 'police corruption', 'acting class', 'skill', 'boyfriend', 'league watcher']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "68it [38:32, 37.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "왕이된남자 ['romance lot', 'saeguk king', 'assassin', 'genius', 'story acting', 'paranoid king', 'barren seon']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "69it [38:46, 30.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "우리들의블루스 ['cast vibe', 'daughter golf', 'bullying', 'romance couple', 'camellia bloom', 'director writer', 'jung eun']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "70it [39:01, 25.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "우수무당가두심 ['fantasy', 'kim sae', 'teenager master', 'chemistry character', \"i'm eps\", 'shamanism', 'vampire slayer']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "71it [39:15, 22.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "우아한친구들 ['jealousy', 'tae hwan', 'owl thanks', 'shower', \"mom's death\", 'dramaboy', 'revenge jung']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "72it [39:32, 20.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "원더우먼 ['romance thriller', 'snippet', 'daughter arsonist', 'surgery face', 'intelligence', 'shitty law', 'everytime']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "73it [39:52, 20.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "월간집 ['jung favourite', 'cast ending', 'struggle lesson', 'cutie pie', 'writer twist', 'rewatch', 'housing crisis']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "74it [40:21, 22.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "유미의세포들 ['romance lot', 'day cell', 'devil judge', \"yumi's life\", 'hyun acting', 'worry hunger', \"characters' thought\"]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "75it [40:41, 21.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "으라차차와이키키 ['love comedy', 'rewatch', 'fortune mother', 'tiring development', 'lot bakery', 'drunk', 'plot twist']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "76it [41:01, 21.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "의사요한 ['love doctor', 'bait viewer', \"sung's acting\", 'badass bunny', 'death pain', 'writer problem', 'vampire detective']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "77it [41:22, 21.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "이구역의미친X ['rewatch', 'comedy illness', 'romance pacing', 'management paranoia', 'pickpocket', 'girl dog', 'covenience store']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "78it [41:46, 22.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "이리와안아줘 ['killer brother', 'chatterbox girl', 'preference melodrama', 'home dog', 'parent lack', 'moon lover', 'tension suspense']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "79it [42:14, 23.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인간수업 ['thriller twist', 'girl parent', 'class acting', 'eat dog', 'bullying sex', 'thought day', 'lawless lawyer']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "80it [42:28, 20.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "자백 ['perfection actress', 'war son', 'plot twist', 'flower evil', \"ho's chemistry\", 'hyun heart', 'rest cast']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "81it [42:41, 18.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "작은신의아이들 ['mystery plot', 'byul popeye', 'dramamilk com', 'dyingtobeinkorea', 'lot people', 'grandmother', 'rest']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "82it [43:26, 26.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "지옥 ['horror demon', \"jungmin's\", 'expectation brainwash', 'politics abortion', 'father destiny', 'magic', 'nature event']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "83it [43:49, 25.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "카이로스 ['romance vibe', 'tantrum home', 'kairos plot', 'logic', 'effect thriller', 'staff cast', 'wife bitch']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "84it [44:21, 27.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "킹덤 ['palace intrigue', 'dyingtobeinkorea zombie', 'actress bae', 'physician father', 'day break', 'prison playbook', 'cinematography acting']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "85it [44:34, 23.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "태종이방원 ['animal cruelty', 'entertainment http', 'death vain', \"i'm dragon\", 'politics', 'stunt injury', 'badass bunny']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "86it [44:47, 20.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "트랩 ['thriller', 'medicine', 'sung', 'ceo hong', 'psychopath', 'taste', 'trap']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "87it [45:01, 18.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "트레이서 ['villain backstories', 'tax office', 'people tracer', 'mbc', 'cliche politician', 'eps', 'luck']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "88it [45:28, 20.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "하늘에서내리는일억개의별 ['thriller mystery', 'one kimura', 'literature theater', 'pasta meat', 'fan ending', 'incest korean', 'jin forgiving']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "89it [45:46, 20.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "하이에나 ['romance lot', 'hyena giriboy', 'jerk career', 'actress kim', 'character morality', 'badass chemistry', 'drunk driving']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "90it [46:04, 19.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "한번다녀왔습니다 ['fun family', 'yongju market', 'miscarriage fault', 'character chemistry', 'acting actor', 'hee writer', 'job mom']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "91it [46:56, 29.01s/it]c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
            "c:\\Users\\g8428\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "해피니스 ['happiness thriller', 'mother food', 'doctor yeon', 'reaction pandemic', 'theyre zombie', 'detective baseball', 'cure blood']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "92it [47:45, 31.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "호텔델루나 ['love acting', 'breakfast', 'kdramas ghost', 'hotel eternity', 'chemistry pain', 'thought story', 'sister sister']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# drama titles in csv\n",
        "drama_titles = df.groupby('title',as_index=False).count().title\n",
        "\n",
        "output = pd.DataFrame(None,columns=drama_titles)\n",
        "\n",
        "top_n = 500\n",
        "diversity = 0.6\n",
        "\n",
        "for i,title in tqdm(enumerate(drama_titles)):\n",
        "    reviews = df[df.title == title].reviews.to_list()\n",
        "\n",
        "    # tfidf custom vectorizing\n",
        "    candidates = tfidf_vectorizing(reviews) # reviews = list\n",
        "\n",
        "    # embedding\n",
        "    sentences = ','.join(reviews)\n",
        "    doc_embedding, candidate_embeddings = embedding(sentences, candidates) \n",
        "\n",
        "    # keywords extraction\n",
        "\n",
        "    keywords, keywords_vector = mmr(doc_embedding,candidate_embeddings,candidates,top_n,diversity)\n",
        "\n",
        "    with open(f'{i}_{title}_.pickle', 'wb') as f:\n",
        "        pickle.dump(keywords, f, pickle.HIGHEST_PROTOCOL)\n",
        "        pickle.dump(keywords_vector, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    output[title] = keywords[:7]\n",
        "    print(title, keywords[:7])\n",
        "\n",
        "output = output.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "output=output.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "output.to_csv('0506_ENG_Keywords.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open('title.pickle', 'rb') as f:\n",
        "#     keywords = pickle.load(f)\n",
        "#     keywords_vector = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# KMEANS - 작품별 연관키워드 뽑을 때 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkI_LLo9cKko",
        "outputId": "146fd432-4eaa-416f-9f19-35cd06f3f5a0"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "X = keywords_vector\n",
        "\n",
        "true_k = 6\n",
        "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
        "model.fit(X.T)\n",
        "\n",
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = keywords # = candidates\n",
        "for i in range(true_k):\n",
        "    print(\"[\"),\n",
        "    for ind in order_centroids[i, :20]:\n",
        "        print(f'\"{terms[ind]}\"', end=\",\")\n",
        "    print(\"],\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "Hcp43g99G8uQ",
        "outputId": "c6909ead-a4ce-4b14-9c95-a35121a3178f"
      },
      "outputs": [],
      "source": [
        "x = list(range(1,11))\n",
        "y = []\n",
        "for i in x:\n",
        "    model = KMeans(n_clusters=i, init='k-means++', max_iter=100, n_init=1)\n",
        "    model.fit(X.T)\n",
        "    y.append(model.inertia_)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(x,y,label='linear')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_489sUqS-4Ls"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FJI_C1zmcKkn"
      ],
      "name": "keyword&Kmeans.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ceb52e7625d0a1f58a1a2fe6599c0620217b7c14f62667a485755eb7aa2b8895"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
